{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vehicle Detection With Lane Lines\n",
    "\n",
    "This is the combination of the last two projects, finding and annotating the lane lines with camera correction, and detecting the vehicles on the road.  The code below is simply the two of these projects combined.  Most of it is separate, but the resources that are used and the final classes from each project that assemble the final output are also combined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the libraries used. These were selected to be compatible with both Python 2.x and 3.x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import glob2\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pickle\n",
    "from skimage.feature import hog\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from shapely.geometry import Polygon\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from moviepy.editor import *\n",
    "from IPython.display import HTML\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lane Detection Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class represents an image.  It includes the means of setting a grayscale representation, undistorting the image and transforming the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Image:\n",
    "    \n",
    "    def read_file(self, filename):\n",
    "        image = cv2.imread(filename)\n",
    "        self.image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        self.set_shape_gray()\n",
    "        \n",
    "    def set_image(self, image):\n",
    "        self.image = image\n",
    "        self.set_shape_gray()\n",
    "        \n",
    "    def set_shape_gray(self):\n",
    "        self.shape = (self.image.shape[0], self.image.shape[1])\n",
    "        self.gray = cv2.cvtColor(self.image, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "    def undistort(self, cam_params):\n",
    "        cam_matrix = cam_params[0]\n",
    "        cam_dist = cam_params[1]\n",
    "        self.image = cv2.undistort(self.image, cam_matrix, cam_dist, None, cam_matrix)\n",
    "        self.set_shape_gray()\n",
    "    \n",
    "    def transform(self, old_points, new_points):\n",
    "        src = np.array(old_points, np.float32)\n",
    "        dst = np.array(new_points, np.float32)\n",
    "        sx = self.shape[0]\n",
    "        sy = self.shape[1]\n",
    "        img_map = cv2.getPerspectiveTransform(src, dst)\n",
    "        self.image = cv2.warpPerspective(self.image, img_map, (sy,sx), flags=cv2.INTER_LINEAR)\n",
    "        self.set_shape_gray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class simply is an iterator for loading images.  It is used to load the chessboard calibration images for camera calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Images:\n",
    "    \n",
    "    def __init__(self, directory):\n",
    "        self.images = []\n",
    "        filenames = os.listdir(directory)\n",
    "        for filename in filenames:\n",
    "            image = Image()\n",
    "            image.read_file(directory + '/' + filename)\n",
    "            self.images.append(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class takes an array of images and performs camera calibration.  The results are constants that can be used as arguments to the `undistort` method in the `Image` class to undistort particular images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Calibrate:\n",
    "    \n",
    "    def __init__(self, images, calibration_dimensions):\n",
    "        self.images = images\n",
    "        self.cal_dim = calibration_dimensions\n",
    "        \n",
    "    def image_points(self):\n",
    "        image_points = []\n",
    "        for i in range(len(self.images)):\n",
    "            image = self.images[i]\n",
    "            gray = image.gray\n",
    "            self.img_dim = image.shape\n",
    "            ret, corners = cv2.findChessboardCorners(gray, self.cal_dim, None)\n",
    "            if ret == True:\n",
    "                image_points.append(corners)\n",
    "        return np.array(image_points, np.float32)\n",
    "    \n",
    "    def object_points(self, num_points):\n",
    "        object_points = []\n",
    "        temp = []\n",
    "        for v in range(self.cal_dim[1]):\n",
    "            for h in range(self.cal_dim[0]):\n",
    "                temp.append([[h, v, 0]])\n",
    "        for z in range(num_points):\n",
    "            object_points.append(temp)\n",
    "        return np.array(object_points, np.float32)\n",
    "    \n",
    "    def calibrate(self):\n",
    "        img = self.image_points()\n",
    "        obj = self.object_points(len(img))\n",
    "        dim = (self.img_dim[1], self.img_dim[0])\n",
    "        ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(obj, img, dim, None, None)\n",
    "        return (mtx, dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class evaluates a Sobel filter along each axis of the image, and computes a magnitude image and directional image from this that have a threshold applied. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sobel:\n",
    "    \n",
    "    def __init__(self, image):\n",
    "        self.image = image.gray\n",
    "        \n",
    "    def x(self):\n",
    "        return cv2.Sobel(self.image, cv2.CV_64F, 1, 0)\n",
    "    \n",
    "    def y(self):\n",
    "        return cv2.Sobel(self.image, cv2.CV_64F, 0, 1)\n",
    "        \n",
    "    def scale(self, image):\n",
    "        scale = np.max(image) / 255\n",
    "        return (image / scale).astype(np.uint8)\n",
    "    \n",
    "    def threshold(self, image, threshold_param):\n",
    "        _, b = cv2.threshold(image, threshold_param[0], threshold_param[1], cv2.THRESH_BINARY)\n",
    "        return b\n",
    "    \n",
    "    def mag_threshold(self, threshold_params):\n",
    "        x = self.x()\n",
    "        y = self.y()\n",
    "        m = np.sqrt(x**2 + y**2)\n",
    "        s = self.scale(m)\n",
    "        return self.threshold(s, threshold_params)\n",
    "        \n",
    "    def dir_threshold(self, threshold_params):\n",
    "        x = self.x() \n",
    "        y = self.y()\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            d = np.absolute(np.arctan(y / x))\n",
    "            b = np.zeros_like(d)\n",
    "            b[(d > threshold_params[0]) & (d < threshold_params[1])] = 1\n",
    "        return b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class simply converts to the HLS color space and provides methods that return the H, L and S channels, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Color:\n",
    "    \n",
    "    def __init__(self, image):\n",
    "        self.rgb = image.image\n",
    "        self.hls = cv2.cvtColor(self.rgb, cv2.COLOR_RGB2HLS)\n",
    "        \n",
    "    def h(self, threshold_param):\n",
    "        h = self.hls[:,:,0]\n",
    "        _, b = cv2.threshold(h, threshold_param[0], threshold_param[1], cv2.THRESH_BINARY)\n",
    "        return b\n",
    "    \n",
    "    def l(self, threshold_param):\n",
    "        l = self.hls[:,:,1]\n",
    "        _, b = cv2.threshold(l, threshold_param[0], threshold_param[1], cv2.THRESH_BINARY)\n",
    "        return b\n",
    "    \n",
    "    def s(self, threshold_param):\n",
    "        s = self.hls[:,:,2]\n",
    "        _, b = cv2.threshold(s, threshold_param[0], threshold_param[1], cv2.THRESH_BINARY)\n",
    "        return b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histograms are the root of being able to robustly determine the starting position, from the bottom of the frame, of the lane lines.  This class computes the histogram of the bottom half of the frame in terms of the number of detected pixes along each column of the that portion of the image.  The histogram data is converted to counts and positions so that we can use clustering to help identify the centers of the histogram.  The histogram first has a Gaussian profile applied to it, the centers being the estimated center of the lane line next to the car.  The reason for the profile is this helps filter noise in the middle of the lane and the edges.  Once the centroids have been detected using k-means, windows are returned surrounding them that are used to project the lane lines to the horizon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Histogram:\n",
    "    \n",
    "    def __init__(self, image):\n",
    "        self.image = image\n",
    "        self.histogram = np.sum(image[image.shape[0]/2:,:], axis=0)\n",
    "        \n",
    "    def add_image(self, image):\n",
    "        self.image = image\n",
    "        h = self.histogram\n",
    "        h = np.add(h, np.sum(image[image.shape[0]/2:,:], axis=0))\n",
    "        self.histogram = h\n",
    "        \n",
    "    def gaussian_histogram(self, center, scale):\n",
    "        histogram = self.histogram\n",
    "        for i, x in enumerate(histogram):\n",
    "            diff1 = float(i) - float(center[0])\n",
    "            result1 = math.exp(float(scale) * -math.fabs(diff1**2.0))\n",
    "            diff2 = float(i) - float(center[1])\n",
    "            result2 = math.exp(float(scale) * -math.fabs(diff2**2.0))\n",
    "            histogram[i] = int(float(x) * (result1+result2))\n",
    "        return histogram\n",
    "        \n",
    "    def peaks(self, threshold, histogram_range, gaussian_params):\n",
    "        h = self.gaussian_histogram(gaussian_params[0], gaussian_params[1])\n",
    "        h = h.reshape(-1,1)\n",
    "        h = h[histogram_range[0]:histogram_range[1]]\n",
    "        count = 0\n",
    "        new = []\n",
    "        for x in h:\n",
    "            if x > threshold:\n",
    "                new.append(count)\n",
    "            count += 1\n",
    "        h = np.array(new, np.float32)\n",
    "        h = h.reshape(-1,1)\n",
    "        km = KMeans(2)\n",
    "        km.fit(h)\n",
    "        values = km.cluster_centers_.squeeze()\n",
    "        values = np.sort(np.array(values))\n",
    "        values = [values[0] + histogram_range[0], values[1] + histogram_range[0]]\n",
    "        return values\n",
    "        \n",
    "    def windowed_peaks(self, width, threshold, histogram_range, gaussian_params):\n",
    "        p = self.peaks(threshold, histogram_range, gaussian_params)\n",
    "        p1 = (int(p[0] - width), int(p[0] + width))\n",
    "        p2 = (int(p[1] - width), int(p[1] + width))\n",
    "        return (p1, p2)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class is used to track the lane lines as they go to the horizon.  This is done by finding the centroid on the lane information in a windowed region based on the last detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CurveTrack:\n",
    "    \n",
    "    def __init__(self, image):\n",
    "        self.image = image\n",
    "        \n",
    "    def track(self, window, step):\n",
    "        result = []\n",
    "        r = (window[1] - window[0]) / 2\n",
    "        s = self.image.shape\n",
    "        for y in range(s[0]-1, step-1, -step):\n",
    "            totalx = 0\n",
    "            count = 0\n",
    "            ys = y - step/2\n",
    "            for x in range(window[0], window[1]):\n",
    "                if self.image[y][x] > 0:\n",
    "                    totalx = totalx + x\n",
    "                    count += 1\n",
    "            if count != 0:\n",
    "                center = totalx / count\n",
    "                window = (int(center - r), int(center + r))\n",
    "                result.append((center, ys))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class simply takes collections of points, fits a second degree polynomial to them, and then provides methods to estimate points on the resulting polynomial (for generating a smooth curve along the lane line) and computing the radius of curvature of the lane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CurveFit:\n",
    "    \n",
    "    def set_points(self, xs, ys):\n",
    "        self.xs = xs\n",
    "        self.ys = ys\n",
    "        \n",
    "    def set_coeffs(self, coeffs):\n",
    "        self.fit = coeffs\n",
    "    \n",
    "    def fit_quadratic(self):\n",
    "        self.fit = np.polyfit(self.ys, self.xs, 2)\n",
    "    \n",
    "    def estimate_point(self, y):\n",
    "        return self.fit[0]*y**2 + self.fit[1]*y + self.fit[2]\n",
    "        \n",
    "    def compute_radius(self):\n",
    "        y_max = np.max(self.ys)\n",
    "        return ((1 + (2*self.fit[0]*y_max + self.fit[1])**2)**1.5) / np.absolute(2*self.fit[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `LaneFinder` class orchestrates finding the lane lines.  It basically just uses the above classes to compute and overlay the lane markings and summary statistics (curvature, off-center distance) on the corrected frame.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LaneFinder:\n",
    "    \n",
    "    def __init__(self, test_image_folder):\n",
    "        self.test_image_folder = test_image_folder\n",
    "        \n",
    "    def calibrate(self, calibration_folder, calibration_size):\n",
    "        self.images = Images(calibration_folder)\n",
    "        cal = Calibrate(self.images.images, calibration_size)\n",
    "        self.cam_cal = cal.calibrate()\n",
    "     \n",
    "    def load_image(self, filename):\n",
    "        image = Image()\n",
    "        image.read_file(self.test_image_folder + '/' + filename)\n",
    "        self.image = image\n",
    "        if hasattr(self.image, 'caml_cal'):\n",
    "            self.image.undistort(self.cam_cal)\n",
    "        \n",
    "    def set_image(self, image):\n",
    "        self.image.set_image(image)\n",
    "        \n",
    "    def set_video_image(self, image):\n",
    "        self.image = image\n",
    "        \n",
    "    def save_image(self, filename):\n",
    "        image = cv2.cvtColor(self.image.image, cv2.COLOR_BGR2RGB)\n",
    "        cv2.imwrite(filename, image)\n",
    "        \n",
    "    def transform_image(self, source_points, dest_points):\n",
    "        self.image.transform(source_points, dest_points)\n",
    "        \n",
    "    def get_image(self):\n",
    "        return self.image.image\n",
    "        \n",
    "    def filter_image(self, color_threshold, sobel_mag_threshold, sobel_dir_threshold):\n",
    "        color = Color(self.image)\n",
    "        sobel = Sobel(self.image)\n",
    "        l = color.l(color_threshold[1])\n",
    "        s = color.s(color_threshold[2])\n",
    "        m = sobel.mag_threshold(sobel_mag_threshold)\n",
    "        d = sobel.dir_threshold(sobel_dir_threshold)\n",
    "        r = np.multiply(l, s)\n",
    "        r = np.multiply(r, d)\n",
    "        return r\n",
    "    \n",
    "    def track_lane(self, image, window, step):\n",
    "        c = CurveTrack(image)\n",
    "        return c.track(window, step)\n",
    "    \n",
    "    def radius(self, points, which_radius):\n",
    "        z = list(zip(*points))\n",
    "        c = CurveFit()\n",
    "        c.set_points(z[0], z[1])\n",
    "        c.fit_quadratic()\n",
    "        if which_radius == 'left':\n",
    "            self.left_fit = c.fit\n",
    "        if which_radius == 'right':\n",
    "            self.right_fit = c.fit\n",
    "        return c.compute_radius()\n",
    "    \n",
    "    def interpolate(self, image, points, y_range):\n",
    "        z = list(zip(*points))\n",
    "        c = CurveFit()\n",
    "        c.set_points(z[0], z[1])\n",
    "        c.fit_quadratic()\n",
    "        result = []\n",
    "        for y in range(y_range[0], y_range[1]):\n",
    "            x = int(c.estimate_point(y))\n",
    "            if x > 0 and x < image.shape[1]-1 and y > 0 and y < image.shape[0]-1:\n",
    "                result.append((x,y))\n",
    "        return result\n",
    "    \n",
    "    def interpolate_video(self, image, coeffs, y_range):\n",
    "        result = []\n",
    "        c = CurveFit()\n",
    "        c.set_coeffs(coeffs)\n",
    "        for y in range(y_range[0], y_range[1]):\n",
    "            x = int(c.estimate_point(y))\n",
    "            if x > 0 and x < image.shape[1]-1 and y > 0 and y < image.shape[0]-1:\n",
    "                result.append((x,y))\n",
    "        return result\n",
    "    \n",
    "    def apply_contours(self, line_data, left, right, y_range, original_shape):\n",
    "        contour = []\n",
    "        left_contour = np.array(self.interpolate(line_data, left, y_range), np.int32)\n",
    "        left_size,_ = left_contour.shape\n",
    "        right_contour = np.array(self.interpolate(line_data[::-1], right, y_range), np.int32)\n",
    "        right_size,_ = right_contour.shape\n",
    "        min_size = min(left_size, right_size)\n",
    "        left_contour = left_contour[0:min_size]\n",
    "        right_contour = right_contour[0:min_size]\n",
    "        contour.append(left_contour)\n",
    "        contour.append(right_contour[::-1])\n",
    "        contour = np.array(contour, np.int32)\n",
    "        contour = np.reshape(contour, (-1, 2))\n",
    "        curve_overlay_image = np.zeros((original_shape[0], original_shape[1], 3), np.uint8)\n",
    "        cv2.fillPoly(curve_overlay_image, [contour], color=(0,60,0))\n",
    "        cv2.polylines(curve_overlay_image, [left_contour], False, (255,0,0), 30)\n",
    "        cv2.polylines(curve_overlay_image, [right_contour], False, (255,0,0), 30)\n",
    "        return curve_overlay_image\n",
    "    \n",
    "    def apply_contours_video(self, line_data, left_coeff, right_coeff, y_range, original_shape):\n",
    "        contour = []\n",
    "        left_contour = np.array(self.interpolate_video(line_data, left_coeff, y_range), np.int32)\n",
    "        left_size,_ = left_contour.shape\n",
    "        right_contour = np.array(self.interpolate_video(line_data[::-1], right_coeff, y_range), np.int32)\n",
    "        right_size,_ = right_contour.shape\n",
    "        min_size = min(left_size, right_size)\n",
    "        left_contour = left_contour[0:min_size]\n",
    "        right_contour = right_contour[0:min_size]\n",
    "        contour.append(left_contour)\n",
    "        contour.append(right_contour[::-1])\n",
    "        contour = np.array(contour, np.int32)\n",
    "        contour = np.reshape(contour, (-1, 2))\n",
    "        curve_overlay_image = np.zeros((original_shape[0], original_shape[1], 3), np.uint8)\n",
    "        cv2.fillPoly(curve_overlay_image, [contour], color=(0,60,0))\n",
    "        cv2.polylines(curve_overlay_image, [left_contour], False, (255,0,0), 30)\n",
    "        cv2.polylines(curve_overlay_image, [right_contour], False, (255,0,0), 30)\n",
    "        return curve_overlay_image\n",
    "    \n",
    "    def apply_text(self, image, left_radius, right_radius, center_offset):\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        left = '%.1fr' % left_radius\n",
    "        cv2.putText(image, left, (25,700), font, 1, (255,255,255), 2)\n",
    "        right = '%.1fr' % right_radius\n",
    "        cv2.putText(image, right, (1130,700), font, 1, (255,255,255), 2)\n",
    "        center = '%.2fm' % center_offset\n",
    "        cv2.putText(image, center, (600,700), font, 1, (255,255,255), 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class performs averaging, which is used to smooth the output of the lane tracker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AveragingChecker:\n",
    "    \n",
    "    def __init__(self, count):\n",
    "        self.data = []\n",
    "        self.count = count\n",
    "        \n",
    "    def add(self, data):\n",
    "        self.data.append(data)\n",
    "        self.data = self.data[-self.count:]\n",
    "       \n",
    "    def average(self):\n",
    "        return np.mean(self.data, axis=0)\n",
    "    \n",
    "    def check_with_threshold(self, data, threshold):\n",
    "        average = self.average()\n",
    "        diff = np.fabs(np.subtract(data, average))\n",
    "        result = True\n",
    "        for i, x in enumerate(diff):\n",
    "            if x > threshold[i]:\n",
    "                result = False\n",
    "        return result\n",
    "    \n",
    "    def add_with_threshold_check(self, data, threshold):\n",
    "        if self.check_with_threshold(data, threshold):\n",
    "            self.add(data)\n",
    "        return self.average()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vehicle Detection and Tracking Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier is based on a mixture of hue/saturation features from the HLS color space, spatially binned features and HOG features.  The data set is the entirety of the provided data set for the assignment.  You can see in the `compute_spatial_hls_hog` method some ideas on reducing the feature vector to those features with the most significant contributions as noted from the `feature_importances_` property of the decision tree classifier.  Reducing the feature space introduced false-positives, so I did not end up using that approach for the final output.  I save the entire `Classifier` class as a `pickle` object so that I can load it easier in the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "class Classifier:\n",
    "\n",
    "    IMAGE_SIZE = (64, 64)\n",
    "    \n",
    "    SPATIAL_SIZE = (32, 32)\n",
    "    \n",
    "    HOG_ORIENTATIONS = 9\n",
    "    HOG_PIXELS_PER_CELL = (8,8)\n",
    "    HOG_CELLS_PER_BLOCK = (2,2)\n",
    "    \n",
    "    HLS_BINS = 512 \n",
    "    HLS_BIN_RANGE = (0,255)\n",
    "    \n",
    "    SVC_KERNEL = 'rbf'\n",
    "    SVC_C_VALUE = 2.0\n",
    "    \n",
    "    DT_MIN_SAMPLES_SPLIT = 40\n",
    "    \n",
    "    TEST_FRACTION = 0.2\n",
    "    RANDOM_STATE = 12345\n",
    "    \n",
    "    PICKLE_FILE = 'classifier.p'\n",
    "    \n",
    "    def __init__(self, true_path, false_path):\n",
    "        self.true_path = true_path\n",
    "        self.false_path = false_path\n",
    "        \n",
    "    def get_recursive_paths(self, path_def):\n",
    "        return glob2.glob(path_def)\n",
    "\n",
    "    def load_image(self, filename):\n",
    "        image = mpimg.imread(filename)\n",
    "        return cv2.resize(image, self.IMAGE_SIZE) \n",
    "\n",
    "    def spatial_bins(self, image):\n",
    "        return cv2.resize(image, self.SPATIAL_SIZE).ravel() \n",
    "    \n",
    "    def hls_bins(self, image):\n",
    "        hls = cv2.cvtColor(image, cv2.COLOR_RGB2HLS)\n",
    "        h_hist = np.histogram(hls[:,:,0], bins=self.HLS_BINS, range=self.HLS_BIN_RANGE)\n",
    "        s_hist = np.histogram(hls[:,:,2], bins=self.HLS_BINS, range=self.HLS_BIN_RANGE)\n",
    "        return np.concatenate((h_hist[0], s_hist[0]))\n",
    "    \n",
    "    def hog_bins(self, image):\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        return hog(gray, orientations=self.HOG_ORIENTATIONS, pixels_per_cell=self.HOG_PIXELS_PER_CELL, cells_per_block=self.HOG_CELLS_PER_BLOCK, transform_sqrt=True, visualise=False, feature_vector=True)\n",
    "        \n",
    "    def compute_spatial_hls_hog(self, image):\n",
    "        spatial = self.spatial_bins(image)\n",
    "        hls = self.hls_bins(image)\n",
    "        hog = self.hog_bins(image)\n",
    "        return np.concatenate((spatial, hls, hog))\n",
    "        \n",
    "    def normalize(self, data):\n",
    "        data=np.array(data, np.float32)\n",
    "        self.scaler = StandardScaler().fit(data)\n",
    "        scaled_data = self.scaler.transform(data)\n",
    "        return scaled_data\n",
    "    \n",
    "    def train(self):\n",
    "        self.svc_classifier = SVC(kernel=self.SVC_KERNEL,C=self.SVC_C_VALUE)\n",
    "        self.svc_classifier.fit(self.training_features, self.training_labels)\n",
    "        self.dt_classifier = DecisionTreeClassifier(min_samples_split=self.DT_MIN_SAMPLES_SPLIT)\n",
    "        self.dt_classifier.fit(self.training_features, self.training_labels)\n",
    "        return self\n",
    "        \n",
    "    def combine(self, list_a, list_b):\n",
    "        result = []\n",
    "        for idx, val_a in enumerate(list_a):\n",
    "            val_b = list_b[idx]\n",
    "            if val_a == 1.0 or val_b == 1.0:\n",
    "                result.append(1.0)\n",
    "            else:\n",
    "                result.append(0.0)\n",
    "        return result\n",
    "        \n",
    "    def test(self):\n",
    "        svc_prediction = self.svc_classifier.predict(self.testing_features)\n",
    "        svc_accuracy = accuracy_score(svc_prediction, self.testing_labels)\n",
    "        dt_prediction = self.dt_classifier.predict(self.testing_features)\n",
    "        dt_accuracy = accuracy_score(dt_prediction, self.testing_labels)\n",
    "        composite_prediction = self.combine(svc_prediction, dt_prediction)\n",
    "        composite_accuracy = accuracy_score(composite_prediction, self.testing_labels)\n",
    "        return np.array([svc_accuracy, dt_accuracy, composite_accuracy])\n",
    "        \n",
    "    def classify_new(self, image):\n",
    "        image = cv2.resize(image, self.IMAGE_SIZE)\n",
    "        data = self.compute_spatial_hls_hog(image)\n",
    "        data = self.scaler.transform(data)\n",
    "        svc_prediction = self.svc_classifier.predict(data)\n",
    "        dt_prediction = self.dt_classifier.predict(data)\n",
    "        composite_prediction = self.combine(svc_prediction, dt_prediction)\n",
    "        return np.array([svc_prediction, dt_prediction, composite_prediction])\n",
    "        \n",
    "    def load(self):\n",
    "        features = []\n",
    "        labels = []\n",
    "        true_paths = self.get_recursive_paths(self.true_path)\n",
    "        for filename in true_paths:\n",
    "            image = self.load_image(filename)\n",
    "            hls_hog = self.compute_spatial_hls_hog(image)\n",
    "            features.append(hls_hog)\n",
    "            labels.append(1.0)\n",
    "        false_paths = self.get_recursive_paths(self.false_path) \n",
    "        for filename in false_paths:\n",
    "            image = self.load_image(filename)\n",
    "            hls_hog = self.compute_spatial_hls_hog(image)\n",
    "            features.append(hls_hog)\n",
    "            labels.append(0.0)\n",
    "        features = self.normalize(features)\n",
    "        self.training_features, self.testing_features, self.training_labels, self.testing_labels = train_test_split(features, labels, test_size=self.TEST_FRACTION, random_state=self.RANDOM_STATE)\n",
    "        self.training_labels = np.array(self.training_labels, np.float32)\n",
    "        self.testing_labels = np.array(self.testing_labels, np.float32)\n",
    "        return self\n",
    "    \n",
    "    def save_classifier(self):\n",
    "        pickle.dump(self, open(self.PICKLE_FILE, 'wb'))\n",
    "        \n",
    "        \n",
    "classifier = Classifier('vehicles/**/*.png', 'non-vehicles/**/*.png')        \n",
    "classifier.load().train().test()\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class is used to generate proportional steps between a starting and stopping point.  It is used in the next class to generate the tilings, both as a vertical and horizontal iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ProportionalStepScaler:\n",
    "    \n",
    "    def __init__(self, start_point, stop_point, start_size, stop_size, overlap):\n",
    "        self.start_point = start_point\n",
    "        self.stop_point = stop_point\n",
    "        self.start_size = start_size\n",
    "        self.stop_size = stop_size\n",
    "        self.overlap = overlap\n",
    "        self.size_slope = float(self.stop_size - self.start_size) / float(self.stop_point - self.start_point)\n",
    "        \n",
    "    def compute(self):\n",
    "        result = []\n",
    "        i = self.start_point + self.start_size / 2\n",
    "        size = self.start_size\n",
    "        while i < self.stop_point - self.stop_size / 2:\n",
    "            result.append((int(i),int(size)))\n",
    "            size = int(self.start_size + (i - self.start_point) * self.size_slope)\n",
    "            i = int(i + size * (1 - self.overlap))\n",
    "        return result\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class uses the previous class to generate the tilings.  It takes into account the region of interest contour and checks that there is intersection of the tiled sub-frame before allowing it as a valid sub-frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WindowTiling:\n",
    "    \n",
    "    def __init__(self, image, mask_region, top_size, bottom_size, overlap):\n",
    "        self.poly = Polygon(mask_region)\n",
    "        roi = np.array(mask_region)\n",
    "        self.image = self.region_of_interest(image, roi)\n",
    "        self.top_size = top_size\n",
    "        self.bottom_size = bottom_size\n",
    "        self.overlap = overlap\n",
    "        self.left = np.amin(roi, axis=0)[0]\n",
    "        self.right = np.amax(roi, axis=0)[0]\n",
    "        self.top = np.amin(roi, axis=0)[1]\n",
    "        self.bottom = np.amax(roi, axis=0)[1]\n",
    "        \n",
    "    def region_of_interest(self, img, vertices):\n",
    "        mask = np.zeros_like(img)   \n",
    "        if len(img.shape) > 2:\n",
    "            channel_count = img.shape[2]  \n",
    "            ignore_mask_color = (255,) * channel_count\n",
    "        else:\n",
    "            ignore_mask_color = 255\n",
    "        cv2.fillPoly(mask, [vertices], ignore_mask_color)\n",
    "        return cv2.bitwise_and(img, mask)\n",
    "  \n",
    "    def vertical_enumeration(self):\n",
    "        pss = ProportionalStepScaler(self.top, self.bottom, self.top_size, self.bottom_size, self.overlap) \n",
    "        return pss.compute()\n",
    "        \n",
    "    def horizontal_enumeration(self, size):\n",
    "        pss = ProportionalStepScaler(self.left, self.right, size, size, self.overlap) \n",
    "        return pss.compute()\n",
    "        \n",
    "    def generate_regions(self):\n",
    "        result = []\n",
    "        vertical = self.vertical_enumeration()\n",
    "        for v in vertical:\n",
    "            horizontal = self.horizontal_enumeration(v[1])\n",
    "            for h in horizontal:\n",
    "                center = (h[0], v[0])\n",
    "                size = (h[1], v[1])\n",
    "                result.append((center, size))\n",
    "        return result\n",
    "    \n",
    "    def generate_images(self):\n",
    "        result = []\n",
    "        regions = self.generate_regions()\n",
    "        for r in regions:\n",
    "            center = r[0]\n",
    "            size = r[1]\n",
    "            left = int(center[1] - size[1]/2)\n",
    "            right = int(center[1] + size[1]/2)\n",
    "            top = int(center[0] - size[0]/2)\n",
    "            bottom = int(center[0] + size[0]/2)\n",
    "            poly = ((bottom,left), (top,left), (top,right), (bottom,right))\n",
    "            poly = Polygon(poly)\n",
    "            intersects = poly.intersects(self.poly)\n",
    "            if intersects:\n",
    "                sub_image = self.image[left:right,top:bottom]\n",
    "                result.append((center, size, sub_image))\n",
    "        return result\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class generates two types of heat map overlays: monochromatic intensity only, and color.  The overlays are added to the underlying images in other code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class HeatMap:\n",
    "    \n",
    "    def __init__(self, image):\n",
    "        self.image = np.array(image)\n",
    "        self.height = self.image.shape[0]\n",
    "        self.width = self.image.shape[1]\n",
    "    \n",
    "    def mono_overlay(self, points, radius, intensity):\n",
    "        k_size = 3 * radius\n",
    "        kernel = np.ones((k_size, k_size), np.float32) / (k_size**2)\n",
    "        overlay = np.zeros((self.height, self.width), np.uint8)\n",
    "        for p in points:\n",
    "            p = (int(p[0]), int(p[1]))\n",
    "            ol = np.zeros((self.height, self.width), np.uint8)\n",
    "            cv2.circle(ol, p, radius, intensity, -1)\n",
    "            ol = cv2.filter2D(ol, -1, kernel)\n",
    "            cv2.addWeighted(ol, 1.0, overlay, 1.0, 0, overlay)\n",
    "        return overlay\n",
    "    \n",
    "    def color_overlay(self, points, radius, color):\n",
    "        k_size = 3 * radius\n",
    "        kernel = np.ones((k_size, k_size), np.float32) / (k_size**2)\n",
    "        overlay = np.zeros((self.height, self.width, 3), np.uint8)\n",
    "        for p in points:\n",
    "            p = (int(p[0]), int(p[1]))\n",
    "            ol = np.zeros((self.height, self.width, 3), np.uint8)\n",
    "            cv2.circle(ol, p, radius, color, -1)\n",
    "            ol = cv2.filter2D(ol, -1, kernel)\n",
    "            cv2.addWeighted(ol, 1.0, overlay, 1.0, 0, overlay)\n",
    "        return overlay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class processes a still image.  It defines the masking region for the tiling, the size of the top and bottom tilings, how much overlap, etc.  It loads the classifier from the `pickle` file to make it faster to iterate over some of these hyperparameters in developing it.  The only thing it can do to the final frame is contribute the heat map for the particular frame.  The temporal aspect of the heat map for detection and the vehicle tracking is done at a level beyond this class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class StillImage:\n",
    "    \n",
    "    MASK_REGION = ( (750,600), (750,400), (1280,400), (1280,600) )\n",
    "    \n",
    "    TOP_SIZE = 70\n",
    "    BOTTOM_SIZE = 150  \n",
    "    OVERLAP = 0.92\n",
    "    \n",
    "    VEHICLES = 'vehicles/**/*.png'\n",
    "    NON_VEHICLES = 'non-vehicles/**/*.png'\n",
    "    \n",
    "    PICKLE_FILE = 'classifier.p'\n",
    "    \n",
    "    def load_classifier(self):\n",
    "        self.classifier = pickle.load(open(self.PICKLE_FILE, 'rb'))\n",
    "        \n",
    "    def load_file(self, filename):\n",
    "        self.image = mpimg.imread(filename)\n",
    "        \n",
    "    def load_image(self, image):\n",
    "        self.image = image\n",
    "        \n",
    "    def get_image(self):\n",
    "        return self.image\n",
    "        \n",
    "    def swap(self, data):\n",
    "        return (data[1], data[0])\n",
    "        \n",
    "    def car_points(self):\n",
    "        wt = WindowTiling(self.image, self.MASK_REGION, self.TOP_SIZE, self.BOTTOM_SIZE, self.OVERLAP)\n",
    "        windows = wt.generate_images()\n",
    "        points = []\n",
    "        for w in windows:\n",
    "            center = self.swap(w[0])\n",
    "            bounds = w[1]\n",
    "            ll = (int(center[0] - bounds[0] / 2), int(center[1] - bounds[1] / 2))\n",
    "            ur = (int(center[0] + bounds[0] / 2), int(center[1] + bounds[1] / 2))\n",
    "            corners = (ll, ur)\n",
    "            image = w[2]\n",
    "            is_car = self.classifier.classify_new(image)\n",
    "            if is_car[2] == 1.0:\n",
    "                points.append((center, corners))\n",
    "        return points\n",
    "    \n",
    "    def get_region_centers(self, regions):\n",
    "        return [self.swap(r[0]) for r in regions]\n",
    "        \n",
    "    def car_points_heatmap(self, radius, color):\n",
    "        heat_map = HeatMap(self.image)\n",
    "        points = [self.swap(r[0]) for r in self.car_points()]\n",
    "        overlay = heat_map.color_overlay(points, radius, color)\n",
    "        return overlay\n",
    "    \n",
    "    def intensity_heatmap(self, radius, intensity):\n",
    "        heat_map = HeatMap(self.image)\n",
    "        points = [self.swap(r[0]) for r in self.car_points()]\n",
    "        overlay = heat_map.mono_overlay(points, radius, intensity)\n",
    "        return overlay\n",
    "    \n",
    "    def overlay(self, image, alpha):\n",
    "        cv2.addWeighted(image, alpha, self.image, 1.0, 0, self.image)\n",
    "        return self.image\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class combines the temporal intensity heat maps to produce the spatio-temporal representation of a likelihood of a vehicle at a certain point.  The spatio-temporal data is implicitly projected to the two dimensional plane of the frame.  In order to qualify as an identified region (filled countour from a binary image determined from the heat map), the heat map region must have a value greater than `min_bound`, a countour area greater than `min_area`, and be in the top `rect_limit` regions by area.  The latter is present since in this assignment we have a priori knowledge of the maximum number of car threats, and can reduce errors by ranking by detection size and limiting to this number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class OverlayFrameCombiner:\n",
    "    \n",
    "    def __init__(self, num_frames, height, width):\n",
    "        self.num_frames = num_frames\n",
    "        self.frames = []\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        \n",
    "    def add_frame(self, frame):\n",
    "        self.frames.append(frame)\n",
    "        self.frames = self.frames[-self.num_frames:]\n",
    "\n",
    "    def rectangles(self, min_bound, min_area, rect_limit):\n",
    "        centers = []\n",
    "        bounding_boxes = []\n",
    "        image = np.zeros((self.height, self.width, 3), np.uint8)\n",
    "        combined = np.zeros((self.height, self.width), np.uint8)\n",
    "        for f in self.frames:\n",
    "            cv2.addWeighted(f, 1.0, combined, 1.0, 0, combined)\n",
    "        threshold = cv2.threshold(combined, min_bound, 255, cv2.THRESH_BINARY)\n",
    "        contours = cv2.findContours(threshold[1].copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        contours = sorted(contours[1], key=cv2.contourArea, reverse=True)\n",
    "        for c in contours:\n",
    "            area = cv2.contourArea(c)\n",
    "            if area > min_area:\n",
    "                bb = cv2.boundingRect(c)\n",
    "                center = (bb[0] + bb[2] / 2, bb[1] + bb[3] / 2)\n",
    "                centers.append(center)\n",
    "                bounding_boxes.append(bb)\n",
    "        return (centers[:rect_limit], bounding_boxes[:rect_limit])\n",
    "    \n",
    "    def rectangle_overlay(self, image, bounding_boxes, color):\n",
    "        overlay = np.zeros((self.height, self.width, 3), np.uint8)\n",
    "        for bb in bounding_boxes:\n",
    "            cv2.rectangle(overlay, (bb[0], bb[1]), (bb[0] + bb[2], bb[1] + bb[3]), color, 3)\n",
    "        return overlay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vehicle tracking is done using both the `PointTracker` and `ObjectTracker` classes.  The first class is simply helpful in implementing the second.  \n",
    "\n",
    "The idea of the system is that new points representing detected car locations are fed to the system.  If the point is within some threshold distance of a known point then we will add the new point to that point.  Once all such new points are exhausted, if there are any left that were not added we create a new instance of a `PointTracker` object for the new point.  Any existing points that were not updated with new points are updated with `None` to signify that they did not have a detection.  At this point all of the tracked points are evaluated for fitness to continue existing, which is based on whether or not in a maximum number of tracked events they have more than a specified number of non-existant entries.  These points are culled.  If the remaining tracked points have more than a specified number of valid locations registered, the average over all of the registered locations for that point is returned.  Using this system, the average is returned if there is enough to average, and the point is culled from the system if there are too many instances of not meeting the criteria of a new point being within the distance tolerance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PointTracker:\n",
    "    \n",
    "    TOTAL_POINTS = 30\n",
    "    IS_VALID_POINTS = 15\n",
    "    IS_SHOWN_POINTS = 15\n",
    "    \n",
    "    def __init__(self, center):\n",
    "        self.updated = True\n",
    "        self.centers = [center]\n",
    "   \n",
    "    def reset_update(self):\n",
    "        self.updated = False\n",
    "\n",
    "    def update(self, center):\n",
    "        self.centers.append(center)\n",
    "        self.centers = self.centers[-self.TOTAL_POINTS:]\n",
    "        self.updated = True\n",
    "        \n",
    "    def is_valid(self):\n",
    "        if self.centers.count(None) > self.IS_VALID_POINTS:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "    \n",
    "    def is_shown(self):\n",
    "        if len(self.centers) - self.centers.count(None) > self.IS_SHOWN_POINTS:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def center_average(self):\n",
    "        total = len(self.centers) - self.centers.count(None)\n",
    "        total_x = 0\n",
    "        total_y = 0\n",
    "        for c in self.centers:\n",
    "            if c != None:\n",
    "                total_x += c[0]\n",
    "                total_y += c[1]\n",
    "        return (int(total_x / total), int(total_y / total))\n",
    "    \n",
    "    \n",
    "class ObjectTracker:\n",
    "    \n",
    "    def __init__(self, distance):\n",
    "        self.objects = []\n",
    "        self.distance = distance\n",
    "        \n",
    "    def is_close(self, a, b):\n",
    "        dx = a[0] - b[0]\n",
    "        dy = a[1] - b[1]\n",
    "        d = math.sqrt(dx**2 + dy**2)\n",
    "        if d < self.distance:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def new_data(self, rectangles):\n",
    "        rect = rectangles\n",
    "        for p in self.objects:\n",
    "            p.reset_update()\n",
    "        for r in list(rect):\n",
    "            for o in self.objects:\n",
    "                if self.is_close(r, o.center_average()) and o.updated == False:\n",
    "                    o.update(r)\n",
    "                    rect.remove(r)\n",
    "        for r in rect:\n",
    "            self.objects.append(PointTracker(r))\n",
    "        for o in self.objects:\n",
    "            if o.updated == False:\n",
    "                o.update(None)\n",
    "        new_objects = []\n",
    "        for o in self.objects:\n",
    "            if o.is_valid():\n",
    "                new_objects.append(o)\n",
    "        self.objects = new_objects\n",
    "        result = []\n",
    "        for o in self.objects:\n",
    "            if o.is_shown():\n",
    "                result.append(o.center_average())\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried a variety of ways of building a bounding box around a car that is based on an estimated center of the vehicle.  Some of the approaches were clustering the marginal counts from a Sobel frame, and using a Sobel frame to define countours and then selecting the longest ones as a means of identifying the outer contour of the vehicle.  These did not work as reliably as the one presented below, which uses image moments after selecting colors and keeping the saturation from the HLS color space.  The bounds are determined by using the image moments along with the top percetile results of a marginal histogram based on the pixel counts in the respective direction.  I found that between the `90th` and `95th` percentile worked pretty well, as this rejects the top few that have offshoots beyond the car and the vast bulk of the lower counts that represent missing parts of the car image from the procedure.  This procedure is also performed in a masked region, as cars near the right of the road have artifacts that skew the bounds from the trees and other similar features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class FilteredCentroidBoundingBox:\n",
    "    \n",
    "    MASK_REGION = ( (750,600), (750,400), (1280,400), (1280,600) )\n",
    "    \n",
    "    LOW_COLOR = (0,0,0)\n",
    "    HIGH_COLOR = (255,255,255)\n",
    "    \n",
    "    THRESHOLD_MIN = 50\n",
    "    THRESHOLD_MAX = 255\n",
    "    \n",
    "    LOW_PERCENTILE = 90\n",
    "    HIGH_PERCENTILE = 95\n",
    "    \n",
    "    def __init__(self, image):\n",
    "        self.image = image\n",
    "        self.compute_image = image\n",
    "        \n",
    "    def region_of_interest(self, img, vertices):\n",
    "        mask = np.zeros_like(img)   \n",
    "        if len(img.shape) > 2:\n",
    "            channel_count = img.shape[2]  \n",
    "            ignore_mask_color = (255,) * channel_count\n",
    "        else:\n",
    "            ignore_mask_color = 255\n",
    "        cv2.fillPoly(mask, [np.array(vertices)], ignore_mask_color)\n",
    "        return cv2.bitwise_and(img, mask)\n",
    "\n",
    "    def select_colors(self, low_value, high_value):\n",
    "        mask = cv2.inRange(self.compute_image, low_value, high_value)\n",
    "        self.compute_image = cv2.bitwise_and(self.compute_image, self.compute_image, mask=mask)\n",
    "    \n",
    "    def select_mask_region(self, region):\n",
    "        self.compute_image = self.region_of_interest(self.compute_image, region)\n",
    "    \n",
    "    def mutate_colorspace(self):\n",
    "        hls = cv2.cvtColor(self.compute_image, cv2.COLOR_RGB2HLS)\n",
    "        s = hls[:,:,2]\n",
    "        self.compute_image = s\n",
    "        \n",
    "    def select_region(self, region):\n",
    "        self.compute_image = self.compute_image[region[0][0]:region[0][1],region[1][0]:region[1][1]]\n",
    "    \n",
    "    def scale_and_threshold(self, threshold_min, theshold_max):\n",
    "        scale = np.max(self.compute_image) / 255.0\n",
    "        self.compute_image = (self.compute_image / scale).astype(np.uint8)\n",
    "        _, self.compute_image = cv2.threshold(self.compute_image, threshold_min, theshold_max, cv2.THRESH_BINARY)\n",
    "    \n",
    "    def compute_centroid(self):\n",
    "        m = cv2.moments(self.compute_image)\n",
    "        x = int(m[\"m10\"] / m[\"m00\"])\n",
    "        y = int(m[\"m01\"] / m[\"m00\"])\n",
    "        return (x, y)\n",
    "    \n",
    "    def compute_weighted_marginal(self, axis):\n",
    "        data = np.sum(self.compute_image, axis=axis) / 255\n",
    "        data = np.sort(data)\n",
    "        length = len(data)\n",
    "        low = int(length * (self.LOW_PERCENTILE - 100) / 100)\n",
    "        high = int(length * (self.HIGH_PERCENTILE - 100) / 100)\n",
    "        data = data[low:high]\n",
    "        return np.average(data)\n",
    "    \n",
    "    def get_bbox(self, center_estimate, bounds):\n",
    "        self.select_colors(self.LOW_COLOR, self.HIGH_COLOR)\n",
    "        self.select_mask_region(self.MASK_REGION)\n",
    "        self.mutate_colorspace()\n",
    "        x_range = (center_estimate[0] - bounds[0], center_estimate[0] + bounds[0])\n",
    "        y_range = (center_estimate[1] - bounds[1], center_estimate[1] + bounds[1])\n",
    "        region = (x_range, y_range)\n",
    "        self.select_region(region)\n",
    "        self.scale_and_threshold(self.THRESHOLD_MIN, self.THRESHOLD_MAX)\n",
    "        center = self.compute_centroid()\n",
    "        x = self.compute_weighted_marginal(1)\n",
    "        y = self.compute_weighted_marginal(0)\n",
    "        x1 = int(center[0] - x / 2 + center_estimate[1] - bounds[0])\n",
    "        x2 = int(center[0] + x / 2 + center_estimate[1] - bounds[0])\n",
    "        y1 = int(center[1] - y / 2 + center_estimate[0] - bounds[1])\n",
    "        y2 = int(center[1] + y / 2 + center_estimate[0] - bounds[1])\n",
    "        return (x1,y1,x2-x1,y2-y1)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code For Running the Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class has methods for computing the two types of output, lane detection and vehicle detection, and overlaying them on the compensated video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LaneLinesAndVehicleDetection:\n",
    "    \n",
    "    CALIBRATION_DIRECTORY     = 'camera_cal'\n",
    "    VIDEO_DIRECTORY           = 'test_videos'\n",
    "\n",
    "    CALIBRATION_GEOMETRY      = (9,5)\n",
    "    CAMERA_COORDINATES        = [[750,450], [1280,600], [0,600], [550,450]]\n",
    "    BIRDS_EYE_COORDINATES     = [[1180,100],  [1180,620], [100,620], [100,100]]\n",
    "    COLOR_THRESHOLD           = [(150,220), (120,250), (50,250)] \n",
    "    SOBEL_MAG_THRESHOLD       = (100, 250)\n",
    "    SOBEL_DIR_THRESHOLD       = (0.2, 0.9)  \n",
    "    GAUSSIAN_PARAMS           = ((500, 800), 0.00001) \n",
    "    HISTOGRAM_VALID_RANGE     = (300, 980)\n",
    "    WINDOWED_PEAKS_ARG        = (100, 50)  \n",
    "    Y_RANGE_CURVE_MAPPED      = (250, 710)\n",
    "    VERTICAL_TRACK_WIDTH      = 10\n",
    "    LANE_CENTER_LOCATION      = 640\n",
    "    LANE_CENTER_SCALE         = 0.004  \n",
    "    VIDEO_AVERAGE_FRAMES      = 10\n",
    "    CURVE_THRESHOLD           = (1.0e-03, 1.0e0, 1.2e+02)\n",
    "    \n",
    "    FRAME_WIDTH               = 1280\n",
    "    FRAME_HEIGHT              = 720\n",
    "    OVERLAY_FRAMES            = 7\n",
    "    PLOT_HEATMAP_COLOR        = (100,100,0)\n",
    "    PLOT_HEATMAP_SIZE         = 20\n",
    "    COMPUTE_HEATMAP_INTENSITY = 20\n",
    "    COMPUTE_HEATMAP_SIZE      = 35\n",
    "    RECTANGLE_LIMIT           = 2\n",
    "    RECTANGLE_MIN_INTENSITY   = 60\n",
    "    RECTANGLE_MIN_AREA        = 7000\n",
    "    BOUNDING_BOX_COLOR        = (255,0,0)\n",
    "    OBJECT_TRACKER_DISTANCE   = 100\n",
    "    HEATMAP_ALPHA             = 0.4\n",
    "    INITIAL_BOUNDS            = (100,100)\n",
    "    \n",
    "    def __init__(self, input_video_name, output_video_name):\n",
    "        self.input_video_name = input_video_name\n",
    "        self.output_video_name = output_video_name\n",
    "        self.si = StillImage()\n",
    "        self.si.load_classifier()\n",
    "        self.ofc = OverlayFrameCombiner(self.OVERLAY_FRAMES, self.FRAME_HEIGHT, self.FRAME_WIDTH)\n",
    "        self.ot = ObjectTracker(self.OBJECT_TRACKER_DISTANCE)\n",
    "    \n",
    "    def process_frame_lane(self, frame, lane_finder, initialization):\n",
    "        lf = lane_finder\n",
    "        image = Image()\n",
    "        image.set_image(frame)\n",
    "        lf.set_video_image(image)\n",
    "        if hasattr(lf.image, 'caml_cal'):\n",
    "            lf.image.undistort(lf.cam_cal)\n",
    "        original_image = lf.get_image()\n",
    "        original_shape = (original_image.shape[0], original_image.shape[1])\n",
    "        lf.transform_image(self.CAMERA_COORDINATES, self.BIRDS_EYE_COORDINATES)\n",
    "        raw_transformed_image = lf.get_image()\n",
    "        line_data_frame = lf.filter_image(self.COLOR_THRESHOLD, self.SOBEL_MAG_THRESHOLD, self.SOBEL_DIR_THRESHOLD)\n",
    "        h = Histogram(line_data_frame)\n",
    "        peaks = h.windowed_peaks(self.WINDOWED_PEAKS_ARG[0], self.WINDOWED_PEAKS_ARG[1], self.HISTOGRAM_VALID_RANGE, self.GAUSSIAN_PARAMS)\n",
    "        left = lf.track_lane(line_data_frame, peaks[0], self.VERTICAL_TRACK_WIDTH)\n",
    "        try:\n",
    "            left_r = lf.radius(left, 'left')\n",
    "        except:\n",
    "            left_r = float('nan')\n",
    "        right = lf.track_lane(line_data_frame, peaks[1], self.VERTICAL_TRACK_WIDTH)\n",
    "        try:\n",
    "            right_r = lf.radius(right, 'right')\n",
    "        except:\n",
    "            right_r = float('nan')\n",
    "        if initialization == True:\n",
    "            self.left_ac.add(lf.left_fit)\n",
    "            self.right_ac.add(lf.right_fit)\n",
    "            return None\n",
    "        else:\n",
    "            lf.left_fit = self.left_ac.add_with_threshold_check(lf.left_fit, self.CURVE_THRESHOLD)\n",
    "            lf.right_fit = self.right_ac.add_with_threshold_check(lf.right_fit, self.CURVE_THRESHOLD)\n",
    "            contour_image = lf.apply_contours_video(line_data_frame, lf.left_fit, lf.right_fit, self.Y_RANGE_CURVE_MAPPED, original_shape)\n",
    "            lf.set_image(contour_image)\n",
    "            lf.transform_image(self.BIRDS_EYE_COORDINATES, self.CAMERA_COORDINATES)\n",
    "            result = cv2.addWeighted(lf.get_image(), 1.0, original_image, 1.0, 0.0)\n",
    "            center_offset = -self.LANE_CENTER_SCALE * (np.average(peaks) - self.LANE_CENTER_LOCATION)\n",
    "            lf.apply_text(result, left_r, right_r, center_offset)\n",
    "            return (original_image, result)\n",
    "        \n",
    "    def process_frame_detection(self, frame):\n",
    "        self.si.load_image(frame)\n",
    "        car_points_heatmap = self.si.car_points_heatmap(self.PLOT_HEATMAP_SIZE, self.PLOT_HEATMAP_COLOR)\n",
    "        intensity_heatmap = self.si.intensity_heatmap(self.COMPUTE_HEATMAP_SIZE, self.COMPUTE_HEATMAP_INTENSITY)\n",
    "        self.ofc.add_frame(intensity_heatmap)\n",
    "        rectangles = self.ofc.rectangles(self.RECTANGLE_MIN_INTENSITY, self.RECTANGLE_MIN_AREA, self.RECTANGLE_LIMIT)\n",
    "        centers = self.ot.new_data(rectangles[0])\n",
    "        rect = []\n",
    "        for c in centers:\n",
    "            fcbb = FilteredCentroidBoundingBox(frame)\n",
    "            bb = fcbb.get_bbox((c[1],c[0]), self.INITIAL_BOUNDS)\n",
    "            rect.append(bb)\n",
    "        rectangles = self.ofc.rectangle_overlay(frame, rect, self.BOUNDING_BOX_COLOR)\n",
    "        return (car_points_heatmap, rectangles)\n",
    "       \n",
    "    def run(self):\n",
    "        lf = LaneFinder(self.VIDEO_DIRECTORY)        \n",
    "        lf.calibrate(self.CALIBRATION_DIRECTORY, self.CALIBRATION_GEOMETRY)\n",
    "        video = VideoFileClip(lf.test_image_folder + '/' + self.input_video_name)\n",
    "        self.left_ac = AveragingChecker(self.VIDEO_AVERAGE_FRAMES)\n",
    "        self.right_ac = AveragingChecker(self.VIDEO_AVERAGE_FRAMES)\n",
    "        output_frames = []\n",
    "        for i, frame in tqdm.tqdm(enumerate(video.iter_frames())):\n",
    "            if i < self.VIDEO_AVERAGE_FRAMES:\n",
    "                self.process_frame_lane(frame, lf, True)\n",
    "            else:\n",
    "                (undistorted_image, lane_frame) = self.process_frame_lane(frame, lf, False)\n",
    "                (heat_map_image, rectangles_image) = self.process_frame_detection(undistorted_image)\n",
    "                cv2.addWeighted(heat_map_image, self.HEATMAP_ALPHA, lane_frame, 1.0, 0, lane_frame)\n",
    "                cv2.addWeighted(rectangles_image, 1.0, lane_frame, 1.0, 0, lane_frame)\n",
    "                output_frames.append(lane_frame)\n",
    "        new_video = ImageSequenceClip(output_frames, fps=video.fps)\n",
    "        new_video.write_videofile(lf.test_image_folder + '/' + self.output_video_name, audio=False)\n",
    "        \n",
    "        \n",
    "llvd = LaneLinesAndVehicleDetection('project_video.mp4', 'project_video_output_combined.mp4')\n",
    "llvd.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:opencv]",
   "language": "python",
   "name": "conda-env-opencv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
