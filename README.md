## __Vehicle Detection and Tracking__


### General Design

Decent software abstraction and design is important for this project, as it has a level of complexity that is more substantial than the prior projects.  I have tried to make the code concise and reasonably articulate as to what the function is, as this is a good project to showcase.  You will find my main submission `VehicleDetection.ipynb` with some comments associated with the classes to supplement the discussion below.  A supplemental submission called `VehicleDetectionWithLaneLines.ipynb` is available that merges the functionality of this assignment with the lane tracking of the previous assignment.  Videos are provided for the results of both of these.

[Screenshot from VehicleDetectionWithLaneLines.ipynb](LaneLinesVehicleDetection.png)



### Feature Detection 

One of the most critical aspects of this project is the feature detection.  That is, to establish accurate detection with minimal false-positives.  The data that I used was the entire collection of provided vehicle and non-vehicle images, separated into a training and test set.  I did write a procedure to extract the vehicle images from the expanded image set from Udacity, but found that a lot of the images were obfuscated or were not cropped well.  These probably work well enough for neural network approaches, but I opted not to take that approach in this assignment, so I did not end up using these data sets for training and testing.  The reason for not taking the neural network approach was that it is clear that speed is of the essence with identifying cars, particularly with a sliding window approach, and a good convnet would be slower in identifying cars.  Moreover, as I will discuss later, using a combination of SVM and decision tree classifiers, I got good performance in classification against the test set.  I pre-scaled the images to 64x64 and normalized them before performing classification.

What I found that worked best for me was a combination of spatially binned RGB data, hue and saturation data after HLS conversion, and HOG data, all in roughly equal portions.  It is visually apparent that the hue information is of great use, as the cars are distinctly different hues than most of the other terrain.  However, this alone gives false-positives with trees and with the white dashed lines.  In practice I found that the combination of hue and saturation helped remedy some of the false-positives with things like trees.  As well, geometric data from the HOG procedure further helped the classification, particularly with regard to false-positive rejection.  As it turns out, simply adding in the direct spatially binned data made a significant difference as well, again with regard to false-positive rejection.  

Regarding the selection of HOG parameters, I found that 8 pixels per cell in each axis, and two cells per block in each axis, along with using nine orientations, worked the best.  This was found by varying the parameters and manually tuning them against the accuracies on the test set.  The reason the pixels per cell and cells per block were good, I believe, is because of the relative number of elements this provided compared to the number of hue/saturation and spatially binned elements.  As to the reason the nine orientations worked best, I am not sure.  My first thought is because it is relatively prime to powers of two, so does not suffer artifact issues.  However, that would be true of 7 as well, and 7 did not performa as well.  Dalal and Triggs, in their original paper on the topic, note that nine worked well for them as well, and also do not offer a convincing reason for why that is.

It is possible to observe the feature importances for the decision tree by observing the `feature_importances_` property after it is trained.  This is graphed in my project.  While there are a lot of areas with very small apparent importances, these turn out to actually be quite important in reducing false-positives.  You can see some commented out code in my `compute_spatial_hls_hog` method that reduces the feature space to those features with more substantial contributions.  This does improve the speed of classification due to reducing the feature vector size, but it is more error prone.  I ended up leaving all of the features for my final submission.

Once the SVM and decision tree classifiers have produced a result for a given image, I determine a combined result by logically ORing them.  The reason for this is that it appears that this produces better results than either classifier alone.  That is to say, performing this operation reduces missed positive features more than it adds false-positives.

Since it is time consuming to traing the classifiers, I save the entire classification class to disk using `pickle` so that it is easy to use at a later time by the image processing classes.



### Pipeline Overview

The general idea of the pipeline is as follows:

 1. Constrain the field of vision to a polygon that roughly defines the lanes to the right of the our car and only extends to the mid-range horizon.  Reason for this is these are the more important vehicles to identify that are a risk, and there is no need to waste time on the rest of the frame. The far horizon is excluded because those cars are not threats.
 2. With this polygon create tiled square sub-regions.  The regions at the top portion of the frame are smaller than those at the bottom. In my implementation they scale linearly between.  This is clearly not the most accurate way of doing this at the far horizon, but we are not evaluating the far horizon as those cars are not threats.  They also overlap left to right, but retain the same size per row.
 3. For each region, determine if it is or is not a car.  The region is scaled and evaluated by the classifier.
 4. On a blank overlay, draw a color heat map of the centers of the regions that are classified as cars for display on a per-frame basis on the full frame.  In the resulting video, these are the yellow marks.
 5. On a separate blank overlay, draw an intensity heat map of the centers of the regions that are classified as cars.  This heat map uses larger areas than the first, and of lower intensity.  The reason for this is that we will be computing with it not only spatially, but across several consecutive frames, and do not want to saturate it.
 6. Over the course of several frames, the intensity heat maps are combined.  The resulting heat map is one taking into account spatial and temporal identifications of cars.  The idea being that the highest density spatio-temporal volumes are the best estimates for the cars.  This is then projected to the two dimensional image to identify the location.
 7. A threshold is performed on the spatio-temporal heat map, which is then used to determine the largest regions by area.  We only keep a maximum number of these regions, and only those larger than a pre-defined area.  The centers are retained to be used for region consistency detection and tracking.
 8. The detected regions from the above procedure are fed to a mechanism that tracks if we have seen the region (whether it is within some distance tolerance or not of a currently known region).  This allows us to apply averaging to those regions we have seen and the ability to provide better confidence in detection to these regions by only allowing the display of those regions with a certain likelihood of repeat detection.
 9. Once the centers have been detected and averaged, the bounding box is determined.  The bounding box is determined using an initial starting point and a bound size guess (constant), and a method that uses binary image moments and specific percentile intervals of the marginal lengths (height and width) of the binary image to refine an estimate of the bounding box.  The image that is used is made binary by selecting colors in the saturation portion of HLS color space.
 10. The bounding box is overlayed on the video frame.  This is the red box in the frame.  



### Results and Opportunities For Improvement

This assignment is pretty hard to get to work well.  It certainly gave me a new appreciation for what Drew Gray said about the problems at Otto with false-positive rejection.  The things that are hard are getting the best classifier possible at the early stages of detection so that you can decrease the number of regions detected with the sliding window approach.  Also, getting the spatio-temporal detection to work well, particularly with tracking the identified regions, can be tricky to get right.  It is also rather hard to get a simple sliding window mechanism to generate good boundaries for the vehicle.  I tried a lot of different ways to improve the boundaries (clustering Sobel frames to detect the edges, selecting the longest contours from binary images based on Sobel frames).  In the end I used a bounding box that uses the image moments on the color space filtered binary image, as this was the most robust of the options that I tried.  

One of the biggest downfalls of this method of vehicle detection is its sensitivity to the tiling that is used.  Since we are using overlapping tiling and depending on that overlap to reinforce classification results, the approach is quite sensitive to the effects of the size of the tiling and the overlap percentage with regard to the time it takes to process a frame.  I feel that I do not get adequate results without depending on fairly significant overlap in the tiling, and with regions that are selected to be roughly the size of the car.  There are diminishing returns with respect to accuracy as the rate that we can process frames slows, but to get an acceptable accuracy using this approach we must do this.  

One implication of the above thoughts on speed is that the training data and/or classification mechanism is is to blame for bad performance.  This data could be reviewed or augmented to help train the classifier better.  The examples from the training data do not, for example, have a lot of white cars, so it is no surprise that the white car is the one that proves harder to track.  Some options that could improve results would be expanding the training set, using augmentation to generate additional training data, further adjusting classification parameters, or using an entirely different approach to classification of the regions. 

While slow, the approach taken works reasonably well.  The white car misses identification for two reasons.  First, when it is on the white pavement, the contrast between the car and the surroundings is low.  HOG features, which have significant performance in classifying the cars, experience a disadvantage in this area due to the low contrast and reduced ability to produce recognizable spatial features.  Later in the video, as the white car moves to the horizon, the bounding box for the frame intersects the location of the car.  This reduces the likelihood of affirmative detection in any one frame, and consequently reduces the ability of the spatio-temporal heat map to identify a sufficient region to mark as a car.

Areas where this project could be improved are: better baseline classification so that the tiling does not require as much overlap, better tiling interpolation in the vertical direction since linear interpolation does not match the car size as a function of vertical position, enhanced parameter selection to optimize the spatio-temporal heat map used to identify cars, and a better means to identify the bounding box of a car based on an intial guess regarding the center of the car.
